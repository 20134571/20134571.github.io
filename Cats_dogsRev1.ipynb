{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "143WyisJ3Lcr-0-GSwcG1X60B9uEzaZM7",
      "authorship_tag": "ABX9TyO/Fdzacx+py8OHZ0Ru/Fn6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/20134571/20134571.github.io/blob/main/Cats_dogsRev1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GAe2xRzpyCZq"
      },
      "outputs": [],
      "source": [
        "import os, time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO9ACcZu6wD3",
        "outputId": "0396968f-949c-4f01-eafd-c9f459ce96e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir(\"/content/drive/My Drive/Colab Notebooks/Week 12/training_set/training_set\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7tmYsCz7Y2t",
        "outputId": "c98208be-7e03-412e-840f-3ebe3f2c0c56"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dogs', 'cats']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/drive/My Drive/Colab Notebooks/Week 12/training_set/training_set\""
      ],
      "metadata": {
        "id": "a2SbqqX42dEK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/My Drive/Colab Notebooks/Week 12/training_set/training_set\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqeTFmzA2Wpa",
        "outputId": "5086ab4d-2e8a-4170-9e03-cce0f58ad441"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/content/drive/My': No such file or directory\n",
            "ls: cannot access 'Drive/Colab': No such file or directory\n",
            "ls: cannot access 'Notebooks/Week': No such file or directory\n",
            "ls: cannot access '12/training_set/training_set': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mxZaLip1dHY",
        "outputId": "2cec1491-d3a5-497e-ed2f-b2a590241326"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train_ds = datasets.ImageFolder(root=f\"{data_dir}\", transform=train_tfms)\n",
        "test_ds = datasets.ImageFolder(root=f\"{data_dir}\", transform=test_tfms)\n"
      ],
      "metadata": {
        "id": "KuJw9E1K8AnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- CONFIG ----------------\n",
        "data_dir = \"/content/drive/My Drive/Colab Notebooks/Week 12/training_set/training_set\"\n",
        "img_size = 128\n",
        "batch_size = 32\n",
        "epochs = 8\n",
        "lr = 1e-3\n",
        "num_workers = 0\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"âœ… Using device: {device}\")\n",
        "\n",
        "# ---------------- DATA ----------------\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "test_tfms = transforms.Compose([\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "#train_ds = datasets.ImageFolder(root=\".\\data\\dogs_cats\\training_set\", transform=train_tfms)\n",
        "#train_ds = datasets.ImageFolder(root=f\"{data_dir}/training_set\", transform=train_tfms)\n",
        "#test_ds  = datasets.ImageFolder(root=f\"{data_dir}/test_set\",  transform=test_tfms)\n",
        "train_ds = datasets.ImageFolder(root=f\"{data_dir}\", transform=train_tfms)\n",
        "test_ds  = datasets.ImageFolder(root=f\"{data_dir}\",  transform=test_tfms)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "class_names = train_ds.classes\n",
        "num_classes = len(class_names)\n",
        "print(f\"Classes: {class_names}\")\n",
        "\n",
        "# ---------------- MODEL ----------------\n",
        "class CNN3x3_32Filters(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=32, num_classes=2):\n",
        "        super().__init__()\n",
        "        # Single convolutional layer: 3 input channels (RGB), 32 filters, 3x3 kernel\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.act = nn.ReLU(inplace=True)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))   # global average pooling\n",
        "        self.fc = nn.Linear(out_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input shape: [B, 3, 128, 128]\n",
        "        x = self.conv(x)     # -> [B, 32, 128, 128]\n",
        "        x = self.bn(x)\n",
        "        x = self.act(x)\n",
        "        x = self.pool(x)     # -> [B, 32, 1, 1]\n",
        "        x = torch.flatten(x, 1)\n",
        "        logits = self.fc(x)  # -> [B, 2]\n",
        "        return logits\n",
        "\n",
        "model = CNN3x3_32Filters(in_channels=3, out_channels=32, num_classes=num_classes).to(device)\n",
        "print(model)\n",
        "\n",
        "# ---------------- LOSS / OPTIMIZER ----------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# ---------------- TRAIN / EVAL ----------------\n",
        "def train_one_epoch(loader):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    for imgs, labels in loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(imgs)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * imgs.size(0)\n",
        "        correct += (logits.argmax(1) == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    preds_all, labels_all = [], []\n",
        "    for imgs, labels in loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        logits = model(imgs)\n",
        "        loss = criterion(logits, labels)\n",
        "        total_loss += loss.item() * imgs.size(0)\n",
        "        correct += (logits.argmax(1) == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        preds_all.extend(logits.argmax(1).cpu().numpy())\n",
        "        labels_all.extend(labels.cpu().numpy())\n",
        "    return total_loss / total, correct / total, preds_all, labels_all\n",
        "\n",
        "# ---------------- TRAINING LOOP ----------------\n",
        "best_acc = 0.0\n",
        "for epoch in range(1, epochs + 1):\n",
        "    tr_loss, tr_acc = train_one_epoch(train_loader)\n",
        "    te_loss, te_acc, preds, labels = evaluate(test_loader)\n",
        "    if te_acc > best_acc:\n",
        "        best_acc = te_acc\n",
        "        torch.save(model.state_dict(), \"best_cnn3x3_32filters.pth\")\n",
        "    print(f\"Epoch {epoch}/{epochs} | Train {tr_loss:.4f}, Acc {tr_acc:.3f} | Test {te_loss:.4f}, Acc {te_acc:.3f}\")\n",
        "\n",
        "print(f\"\\nâœ… Done. Best test accuracy: {best_acc:.3f}\")\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(labels, preds, target_names=class_names, digits=3))\n",
        "print(\"\\nConfusion matrix:\")\n",
        "print(confusion_matrix(labels, preds))\n",
        "\n",
        "# ----- display the images ---- predicted and true label and the image\n",
        "# ====== HOLD-OUT VISUAL TEST: 5 cats + 5 dogs from test set ======\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "# Helper: get targets robustly across torchvision versions\n",
        "try:\n",
        "    test_targets = test_ds.targets  # torchvision>=0.13\n",
        "except AttributeError:\n",
        "    test_targets = [s[1] for s in test_ds.samples]\n",
        "\n",
        "# Find first 5 indices per class (assumes exactly two classes: class_names[0], class_names[1])\n",
        "cls0_idx = train_ds.class_to_idx[class_names[0]]\n",
        "cls1_idx = train_ds.class_to_idx[class_names[1]]\n",
        "\n",
        "cls0_indices, cls1_indices = [], []\n",
        "for i, t in enumerate(test_targets):\n",
        "    if t == cls0_idx and len(cls0_indices) < 5:\n",
        "        cls0_indices.append(i)\n",
        "    elif t == cls1_idx and len(cls1_indices) < 5:\n",
        "        cls1_indices.append(i)\n",
        "    if len(cls0_indices) == 5 and len(cls1_indices) == 5:\n",
        "        break\n",
        "\n",
        "assert len(cls0_indices) == 5 and len(cls1_indices) == 5, \\\n",
        "    \"Not enough images per class in test set to sample 5 each.\"\n",
        "\n",
        "holdout_indices = cls0_indices + cls1_indices\n",
        "holdout_ds = Subset(test_ds, holdout_indices)\n",
        "holdout_loader = DataLoader(holdout_ds, batch_size=10, shuffle=False, num_workers=0)\n",
        "\n",
        "# Inference on the 10 hold-out images\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    images, labels_true = next(iter(holdout_loader))\n",
        "    images = images.to(device)\n",
        "    logits = model(images)\n",
        "    preds = logits.argmax(dim=1).cpu().numpy()\n",
        "    labels_true = labels_true.numpy()\n",
        "\n",
        "# Plot: 2 rows x 5 cols\n",
        "def tensor_to_img(t):\n",
        "    # t: [C,H,W], no normalization used above, so just permute\n",
        "    img = t.permute(1, 2, 0).cpu().numpy()\n",
        "    # clamp just in case any transforms added later\n",
        "    return img.clip(0, 1)\n",
        "\n",
        "plt.figure(figsize=(16, 6))\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    plt.imshow(tensor_to_img(images[i].cpu()))\n",
        "    pred_name = class_names[preds[i]]\n",
        "    true_name = class_names[labels_true[i]]\n",
        "    title = f\"Pred: {pred_name}\\nTrue: {true_name}\"\n",
        "    plt.title(title, fontsize=10)\n",
        "    plt.axis(\"off\")\n",
        "plt.suptitle(\"Hold-out predictions (5 cats + 5 dogs)\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o18XRqfSz0SM",
        "outputId": "e76162aa-9972-4672-fff1-e4382ae57ea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Using device: cuda\n",
            "Classes: ['cats', 'dogs']\n",
            "CNN3x3_32Filters(\n",
            "  (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (act): ReLU(inplace=True)\n",
            "  (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=32, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model Weights\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/Week 12/best_cnn3x3_32filters.pth\")\n"
      ],
      "metadata": {
        "id": "A4GsSYV31ySG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}